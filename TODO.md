# Final touches

- ✅ Add more mediatypes: Also support any other common static datatype that is utf8 and can be generated by LLM.
- ✅ If contextUrl response is HTML, only allow it under 50kb for now -which all my html is (later use Jina or Screenshots)
- ✅ Have a nice resultpage making it easy to test
- ✅ Test and ensure it responds in desired mediatypes and caches correctly
- ✅ Avoid having improper encoding from context retrieved.
- ✅ tos.html and privacy.html
- ✅ Improved 401 page with proper urldecoding

# Improved API

- Add some grace for 429 to `/base/*`. Besides using exponential backoff, use `x-ratelimit-*` headers indicating when we can use stuff again. Max 5x retry by default
- Install json-ptr that can do wildcard `*` as well. Parse input context with JSON pointer
- Always attach `?prompt=` to the contextUrl so if the context actually requires it, it can use it to provide improved context.

## `POST /base` and `POST /chat/completions`

BASE: Same as GET but JSON input.

Plug in /chat/completions as well that uses the same cache but just regular `output` as output
