# Final touches

- ✅ Add more mediatypes: Also support any other common static datatype that is utf8 and can be generated by LLM.
- ✅ If contextUrl response is HTML, only allow it under 50kb for now -which all my html is (later use Jina or Screenshots)
- ✅ Have a nice resultpage making it easy to test
- ✅ Test and ensure it responds in desired mediatypes and caches correctly
- ✅ Avoid having improper encoding from context retrieved.
- Add some grace for 429 to `/base/*`. Besides using exponential backoff, use `x-ratelimit-*` headers indicating when we can use stuff again. Max 5x retry by default

# JSON Pointers

- Install json-ptr that can do wildcard `*` as well
- Parse input context with JSON pointer
- Parse output with JSON pointer

## `POST /base` and `POST /chat/completions`

BASE: Same as GET but JSON input.

Plug in /chat/completions as well that uses the same cache but just regular `output` as output
